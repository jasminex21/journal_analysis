


from pypdf import PdfReader
from dateutil.parser import parse

import pandas as pd

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from datetime import datetime

import string
from nltk import ngrams


# read in journal data from PDF
# ultimately I want to cache data that has already been processed, and just append new data to it
reader = PdfReader("/home/jasmine/PROJECTS/journal_analysis/data/Journal.pdf")
pages = reader.pages


# function to detect date
def is_date(string, fuzzy=False):

    try: 
        parse(string, fuzzy=fuzzy)
        return True

    except ValueError:
        return False


# dict to store entries; key = date of entry
entries = {}
last_possible_date = ""
for page in pages: 
    # extraction_mode=layout to preserve the layout in the original PDF
    text = page.extract_text(extraction_mode="layout")
    ls = text.split("\n")
    # if the first row in the page is a date...
    possible_date = ", ".join(ls[0].split(", ")[1:]).replace(" ", "")
    if is_date(possible_date):
        last_possible_date = possible_date
        entries[possible_date] = "\n".join(ls[1:]).replace("\n", " ").lstrip()
    else: 
        entries[last_possible_date] += text.replace("\n", " ")


entries


entries.keys()


len(entries)


ENTRIES = pd.DataFrame({
    "date": list(entries.keys()),
    "entry": list(entries.values())
})


ENTRIES


ENTRIES["date"] = pd.to_datetime(ENTRIES["date"], format="%B%d,%Y")


ENTRIES


filtered = ENTRIES[ENTRIES["date"] >= pd.Timestamp(date(2024, 10, 1))]
all_text = " ".join(entry for entry in filtered["entry"]).lower()


all_text


stopwords = ["bc", "abt", "ve", "don", "didn", "ll", "got", 
             "haven", "go", "went", "re", "make", "wasn", 
             "really", "will", "wouldn", "put", "yet",
             "doesn", "took", "quite", "way", "actually", 
             "gonna", "still", "gotta"]


wordcloud = WordCloud(width=1000, height=700, background_color='white', 
                      min_word_length=2, stopwords=set(list(STOPWORDS)+stopwords), #-set(["he", "she", "him", "her"] )
                      max_words=300, margin=5, random_state=21,
                      relative_scaling="auto").generate(all_text) 

plt.figure(figsize=(20, 12))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


wordcloud.to_file("CLOUD.jpg")


ENTRIES.to_csv("/home/jasmine/PROJECTS/journal_analysis/data/OCT10.csv", index=False)


ENTRIES = pd.read_csv("/home/jasmine/PROJECTS/journal_analysis/data/OCT10.csv")
ENTRIES


# doing it the more manual way, via frequency counts


set(string.punctuation) - set("'")


# removing punctuation
ENTRIES["entry"] = ENTRIES["entry"].str.replace(f'[{string.punctuation}]', '', regex=True)


counts = pd.DataFrame(ENTRIES["entry"].str.lower().str.split(expand=True).stack().value_counts())
# counts["word"] = counts.index


def get_words_and_bigrams(text):
    # Split text into words
    words = text.split()
    # Generate bigrams
    bigrams_list = list(ngrams(words, 2))
    # Combine words and bigrams into a single list
    return words + [' '.join(bigram) for bigram in bigrams_list]

# Apply the function to each entry and stack the results
all_words_and_bigrams = ENTRIES["entry"].apply(get_words_and_bigrams).explode()

# Count occurrences of each word/bigram
word_bigram_counts = pd.DataFrame(all_words_and_bigrams.value_counts(), columns=['count'])

print(word_bigram_counts)



